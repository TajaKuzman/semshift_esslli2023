{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2a067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from scipy.stats import spearmanr \n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22166c73",
   "metadata": {},
   "source": [
    "# Load embeddings and a testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d638d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coha1 =  KeyedVectors.load_word2vec_format(datapath(os.getcwd() + '/embeddings/coha1_win10-k5-dim100-ep30-iter1.sgns'),\n",
    "                                           binary=False) \n",
    "coha2 =  KeyedVectors.load_word2vec_format(datapath(os.getcwd() + '/embeddings/coha2_win10-k5-dim100-ep30-iter1.sgns'),\n",
    "                                           binary=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c025cf9",
   "metadata": {},
   "source": [
    "We loaded embeddings trained on the __[English Semeval dataset](https://www.ims.uni-stuttgart.de/en/research/resources/corpora/sem-eval-ulscd-eng/)__, i.e. on two subsets of the Corpus of Historical American English (COHA)\n",
    "\n",
    "`coha1` - embeddings trained on subset from 1810 to 1860\n",
    "\n",
    "`coha2` - embeddings trained on subset from 1960 to 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2982834",
   "metadata": {},
   "source": [
    "The difference between corpora is reflected in difference between embeddings. Lets see, for example, how nearest neibours for word `pilot` changed over time. We can see that in the earlier corpus the word is associated with sea navigation, while in the later corpus the meaning shifted towards aircraft navigator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2e3fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coha1.similar_by_word(\"pilot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34476521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coha2.similar_by_word(\"pilot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374043d2",
   "metadata": {},
   "source": [
    "**Your turn:** think about other English words that radically changed their meaning between the first half of 19th century and the second half of 20th century. Insert them into cells above to test your hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cfed6",
   "metadata": {},
   "source": [
    "You probably noted that word `plane` is presented in the embedding dictionary together with a part of speech tag `plane_nn`. This is because this word belongs to a SemEval testset. The corpus was preprocessed to use only required word forms.\n",
    "\n",
    "Lets now load the whole testset, together with manually annotated change scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ae87d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graded = pd.read_csv(os.getcwd() + '/targets/english/graded.txt', sep=\"\\t\", header=None, names=['word', 'truth'])\n",
    "graded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccf987",
   "metadata": {},
   "source": [
    "# Jaccard distance\n",
    "This method is based on computing Jaccard distance between sets of 10 nearest neighbors of x\n",
    "(by cosine distance) in A and B. The Jaccard distance is computed as a intersection size divided by the union size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition\n",
    "\n",
    "def jaccard(word, emb1 = coha1, emb2 = coha2, nn = 10):\n",
    "    # retrieve nearest neighbors\n",
    "    nn1 = emb1.similar_by_word(word, nn)\n",
    "    nn2 = emb2.similar_by_word(word, nn)\n",
    "    \n",
    "    # this method does not use similarity scores, only lists of words\n",
    "    nn1 = set(n[0] for n in nn1)\n",
    "    nn2 = set(n[0] for n in nn2)\n",
    "    \n",
    "    # compute Jaccard score\n",
    "    jaccard = len(nn1.intersection(nn2)) / len(nn1.union(nn2))\n",
    "    \n",
    "    # in the Semeval dataset change scores are between 0 and 1\n",
    "    # so that 0 means no change, 1 means the highest change\n",
    "    # Jaccard score is reverse, 0 means the smallest overlap, i.e. the strongest change\n",
    "    # thus we return 1 - jaccard as the final change score\n",
    "    \n",
    "    return 1 - jaccard\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c5fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute for each word in the list\n",
    "graded[\"jaccard\"] = graded.apply(lambda row: jaccard(row.word), axis = 1)\n",
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b578165c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate using Spearman Rank Correlation\n",
    "spearmanr(graded.truth, graded.jaccard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2b6f8",
   "metadata": {},
   "source": [
    "**Your turn:** Explore whether over number of nearest neighbors (smaller or greater than 10) would improve the method results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda74e7b",
   "metadata": {},
   "source": [
    "# Global Anchors\n",
    "Here, the the intersection of A and B\n",
    "vocabularies (‘global anchors’, or VAB) is used. The degree of semantic\n",
    "change is defined as the cosine distance between the vector of the cosine\n",
    "similarities of x embedding in A to all words in VAB and the vector of the\n",
    "cosine similarities of x embedding in B to all words in VAB;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c9d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definition\n",
    "def glob_a(word, emb1 = coha1, emb2 = coha2):\n",
    "    \n",
    "    # intersection of two vocabularies\n",
    "    VAB = list(set(emb1.index_to_key).intersection(emb2.index_to_key))\n",
    "    \n",
    "    \n",
    "    # vectors of cosine similarities\n",
    "    v1 = emb1.distances(word, VAB)\n",
    "    v2 = emb2.distances(word, VAB)\n",
    "    \n",
    "    # second-order cosine distance\n",
    "    return float(cosine(v1, v2))\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute for each word in the list\n",
    "graded[\"glob_a\"] = graded.apply(lambda row: glob_a(row.word), axis = 1)\n",
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using Spearman Rank Correlation\n",
    "spearmanr(graded.truth, graded.glob_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90a673",
   "metadata": {},
   "source": [
    "**Your turn:** Why, do you think, the correlation is so low in this case? Would it be possible to improve the method by curating VAB?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1fbb9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Orthogonal alignment\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ca9dd",
   "metadata": {},
   "source": [
    "In the methods above we used word embeddings only indirectly, by computing distances to other words within the same embedding space. This is because embeddings are trained independently and, due to stochastic nature of the training process, are not aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f32eb1",
   "metadata": {},
   "source": [
    "Foe example, nearest neighbors for word 'cloud' are rather similar in `coha1` and `coha2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11063387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coha1.similar_by_word('cloud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16c4c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coha2.similar_by_word('cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31059584",
   "metadata": {},
   "source": [
    "However, if we take a *vector* for this word from the first embedding space and try to find where it is located in the second embedding space, the nearest words look completely irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "coha2.similar_by_vector(coha1['cloud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7186a31",
   "metadata": {},
   "source": [
    "Thus, we need to first *align* embedding spaces so that position of semantically similar words become close across embedding space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba2f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alignment is done using vocabularly intersection\n",
    "VAB = list(set(coha1.index_to_key).intersection(coha2.index_to_key))\n",
    "vectors1=coha1.vectors_for_all(VAB).vectors\n",
    "vectors2=coha2.vectors_for_all(VAB).vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "m = vectors2.T.dot(vectors1)\n",
    "# SVD decomposition\n",
    "u, _, v = np.linalg.svd(m)\n",
    "# Orthogonal transformation of the second matrix that makes it most similar to the first matrix\n",
    "ortho = u.dot(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming embedding space using the orthogonal matrix\n",
    "coha2.vectors = coha2.vectors.dot(ortho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7249aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check that now we can query coha2 embeddings using vectors from coha1 embedding space\n",
    "coha2.similar_by_vector(coha1['cloud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ffae7",
   "metadata": {},
   "source": [
    "Now we can measure the degree of semantic change directly using cosine similarities between vectors from `coha1` and `coha2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05e0c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graded[\"align_cos\"] = graded.apply(lambda row: cosine(coha1[row.word], coha2[row.word]), axis = 1)\n",
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d6a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(graded.truth, graded.align_cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e405f",
   "metadata": {},
   "source": [
    "\n",
    "# Incremental training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259678b",
   "metadata": {},
   "source": [
    "Instead of aligning the models post-factum, we can train them *incrementally*, i.e. by initializaing the model for a later time period with weights from the model trained on an ealier time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coha1i =  KeyedVectors.load_word2vec_format(datapath(os.getcwd() + '/embeddings/coha1init_win10-k5-dim100-ep30-iter1.sgns'),\n",
    "                                           binary=False) \n",
    "coha2i =  KeyedVectors.load_word2vec_format(datapath(os.getcwd() + '/embeddings/coha2init_win10-k5-dim100-ep30-iter1.sgns'),\n",
    "                                           binary=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e8f72",
   "metadata": {},
   "source": [
    "In this case models are already aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f33cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coha2i.similar_by_vector(coha1i['cloud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dcc588",
   "metadata": {},
   "source": [
    "and similarities can be computed directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded[\"init_cos\"] = graded.apply(lambda row: cosine(coha1i[row.word], coha2i[row.word]), axis = 1)\n",
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(graded.truth, graded.init_cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11e4deb",
   "metadata": {},
   "source": [
    "**Your turn**: now we have results from 4 different methods. Can you check, which of these results are most similar to each other? And why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
