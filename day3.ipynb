{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f11022",
   "metadata": {},
   "source": [
    "# Contextualized token embeddings for semantic change detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pylab as plot\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import cosine, cdist\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "from helpers import collate, ContextsDataset\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801fe1f",
   "metadata": {},
   "source": [
    "# Loading the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17248820",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded = pd.read_csv(\"targets/english/graded_nopos.txt\", sep=\"\\t\", header=None,\n",
    "                     names=['word', 'truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fd3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d300083",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = graded.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceafed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Target lemmas: {len(targets)}.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd5604",
   "metadata": {},
   "source": [
    "Again, corpus1 is XIX century English, corpus 2 is XX century English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ca79c6",
   "metadata": {},
   "source": [
    "# Embedding part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddccd8b9",
   "metadata": {},
   "source": [
    "We assume the token embeddings are already extracted using the language model of our choice (BERT, XLM-R, etc).\n",
    "If you are curious, look at the `extract.py` script.\n",
    "Embeddings are stored as Numpy matrices (compressed). They are about 200 MBytes each, so we publish them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59324a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path1= \"embeddings/token_embeddings_corpus1_xlmr.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268221c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = np.load(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ddf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Loaded an array of {len(array1)} entries from {data_path1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e95bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path2 = \"embeddings/token_embeddings_corpus2_xlmr.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array2 = np.load(data_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3cb51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Loaded an array of {len(array2)} entries from {data_path2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009fb674",
   "metadata": {},
   "source": [
    "# Visualizing token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a239924",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"plane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e962dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = array2[word]\n",
    "array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a8f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = PCA(n_components=2)\n",
    "y = embedding.fit_transform(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84359774",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpositions = y[:, 0]\n",
    "ypositions = y[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.clf()\n",
    "plot.scatter(xpositions, ypositions, 5, marker='*', color='green')\n",
    "plot.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "plot.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "plot.title(f\"{word} in {data_path2}\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57855449",
   "metadata": {},
   "source": [
    "Every dot is a 768-dimensional token embeddings projected into 2 dimensions.\n",
    "Can you show both time periods side by side?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389635a",
   "metadata": {},
   "source": [
    "We can show two time periods on one plot as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae182519",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {\"bin1\": array1[word], \"bin2\": array2[word]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[\"bin2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate([embeddings[el] for el in sorted(embeddings)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c81f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92389b1",
   "metadata": {},
   "source": [
    "We want to show usages from different time bins with different colors, thus, we need class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = []\n",
    "for el in sorted(embeddings):\n",
    "    class_labels += [el] * len(embeddings[el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b200e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd13fe",
   "metadata": {},
   "source": [
    "We are projecting all embeddings into 2 dimensions with PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = preprocessing.StandardScaler().fit_transform(x)\n",
    "x_2d = PCA(n_components=2).fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_set = sorted([c for c in set(class_labels)])\n",
    "colors = plot.cm.Dark2(np.linspace(1, 0, len(class_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2443577",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.clf\n",
    "plot.figure(figsize=(15, 15))\n",
    "plot.xticks([]), plot.yticks([])\n",
    "plot.title(f\"{word} in all time bins\\n\", fontsize=20)\n",
    "for year in class_set:\n",
    "    rows = [x == year for x in class_labels]\n",
    "    matrix = x_2d[rows]\n",
    "    plot.scatter(matrix[:, 0], matrix[:, 1], color=colors[class_set.index(year)], marker='*', s=40, label=year)\n",
    "plot.legend(prop={'size': 15}, loc=\"best\")\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6163682",
   "metadata": {},
   "source": [
    "What we will need to be able to inspect the actual usages? How to annotate the dots with the identifiers pointing at real sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2ff43",
   "metadata": {},
   "source": [
    "Anyway, now we would like to use the token embeddings to quantitatively estimate the degree of semantic change. And here comes the..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea586ec",
   "metadata": {},
   "source": [
    "# Aggregating and assessment part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b296a",
   "metadata": {},
   "source": [
    "There are many usages and many token embeddings. We need to somehow *aggregate* them for each time period, before *assessing* the change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd2890",
   "metadata": {},
   "source": [
    "The simplest is the PRT method (comparison of averaged *prototypical* embeddings):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab363547",
   "metadata": {},
   "source": [
    "## PRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87423f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prt_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acd1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sorted(targets):\n",
    "    frequency = np.sum([array1[word].shape[0], array2[word].shape[0]])\n",
    "    vectors1 = array1[word]\n",
    "    vectors2 = array2[word]\n",
    "    vectors = []\n",
    "    for m in [vectors1, vectors2]:\n",
    "        # Aggregation:\n",
    "        vector = np.average(m, axis=0)\n",
    "        vectors.append(vector)\n",
    "    vectors = [preprocessing.normalize(v.reshape(1, -1), norm='l2') for v in vectors]\n",
    "    # Assessment:\n",
    "    shift = 1 - np.dot(vectors[0].reshape(-1), vectors[1].reshape(-1))\n",
    "    prt_predictions.append(shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1badced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded[\"prt_predictions\"] = prt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b08b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e232a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(graded.truth, graded.prt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a345a0",
   "metadata": {},
   "source": [
    "## APD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf186e",
   "metadata": {},
   "source": [
    "Average Pairwise Distance (APD) is a more sophisticated aggreation method. It computes pairwise distances between *all* usages from two time bins and averages these distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c817cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pairwise_distance(word_usages1, word_usages2, metric):\n",
    "    \"\"\"\n",
    "    Computes the mean pairwise distance between two usage matrices.\n",
    "\n",
    "    :param word_usages1: a three-place tuple including, in this order, a usage matrix, a list of\n",
    "    snippets, and a list of integers indicating the lemma's position in the snippet\n",
    "    :param word_usages2: a three-place tuple including, in this order, a usage matrix, a list of\n",
    "    snippets, and a list of integers indicating the lemma's position in the snippet\n",
    "    :param metric: a distance metric compatible with `scipy.spatial.distance.cdist`\n",
    "    (e.g. 'cosine', 'euclidean')\n",
    "    :return: the mean pairwise distance between two usage matrices\n",
    "    \"\"\"\n",
    "    if isinstance(word_usages1, tuple):\n",
    "        usage_matrix1, _, _ = word_usages1\n",
    "    else:\n",
    "        usage_matrix1 = word_usages1\n",
    "\n",
    "    if isinstance(word_usages2, tuple):\n",
    "        usage_matrix2, _, _ = word_usages2\n",
    "    else:\n",
    "        usage_matrix2 = word_usages2\n",
    "\n",
    "    if usage_matrix1.shape[0] == 0 or usage_matrix2.shape[0] == 0:\n",
    "        raise ValueError('Zero-dimensional usage matrix.')\n",
    "    return np.mean(cdist(usage_matrix1, usage_matrix2, metric=metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca008b",
   "metadata": {},
   "source": [
    "Computational complexity naturally grows quadratically with the number of usages, so we will introduce sampling of max 10 000 random usages from each time bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "apd_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b10702",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sorted(targets):\n",
    "    frequency = np.sum([array1[word].shape[0], array2[word].shape[0]])\n",
    "    print(f\"Processing {word} with the frequency {frequency}...\")\n",
    "    vectors1 = array1[word]\n",
    "    vectors2 = array2[word]\n",
    "    if vectors1.shape[0] > max_samples:\n",
    "        prev = vectors1.shape[0]\n",
    "        rand_indices = np.random.choice(prev, max_samples, replace=False)\n",
    "        vectors1 = vectors1[rand_indices]\n",
    "        f\"Choosing {max_samples} random usages from {prev} for {word} in T0\"\n",
    "    if vectors2.shape[0] > max_samples:\n",
    "        prev = vectors2.shape[0]\n",
    "        rand_indices = np.random.choice(prev, max_samples, replace=False)\n",
    "        vectors2 = vectors2[rand_indices]\n",
    "        f\"Choosing {max_samples} random usages from {prev} for {word} in T1\"\n",
    "    shift = mean_pairwise_distance(vectors1, vectors2, \"cosine\")\n",
    "    apd_predictions.append(shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded[\"apd_predictions\"] = apd_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ff351",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5354c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(graded.truth, graded.apd_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a98fb8",
   "metadata": {},
   "source": [
    "## PRT/APD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b66ace",
   "metadata": {},
   "source": [
    "One can also take the geometric mean of PRT and APD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded[\"prt_apd_predictions\"] = graded.apply(lambda row: np.sqrt(row.prt_predictions * row.apd_predictions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearmanr(graded.truth, graded.prt_apd_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cbbb8f",
   "metadata": {},
   "source": [
    "You can try other languages!\n",
    "Annotated datasets are available here: https://www.ims.uni-stuttgart.de/en/research/resources/experiment-data/wugs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
